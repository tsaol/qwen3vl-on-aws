# Qwen3-VL åœ¨ AWS ä¸Šçš„éƒ¨ç½²æŒ‡å—

è¿™ä¸ªä»“åº“åŒ…å«åœ¨ AWS ä¸Šéƒ¨ç½² Qwen3-VL-8B-Instruct æ¨¡å‹çš„è„šæœ¬å’Œæ–‡æ¡£ã€‚

## ç¯å¢ƒè¦æ±‚

- AWS EC2 å®ä¾‹ï¼ˆæ¨è G5 æˆ– P ç³»åˆ— GPU å®ä¾‹ï¼‰
- Python 3.10+
- NVIDIA GPUï¼ˆæ”¯æŒ CUDAï¼‰
- Ubuntu 22.04 æˆ–æ›´é«˜ç‰ˆæœ¬

## æ¨èçš„ AWS å®ä¾‹ç±»å‹

| å®ä¾‹ç±»å‹ | GPU | æ˜¾å­˜ | é€‚ç”¨åœºæ™¯ |
|---------|-----|------|---------|
| g5.xlarge | 1x A10G | 24GB | å°è§„æ¨¡æµ‹è¯• |
| g5.2xlarge | 1x A10G | 24GB | å¼€å‘ç¯å¢ƒ |
| p3.2xlarge | 1x V100 | 16GB | è®­ç»ƒ/æ¨ç† |
| p4d.24xlarge | 8x A100 | 320GB | å¤§è§„æ¨¡ç”Ÿäº§ |

## å¿«é€Ÿå¼€å§‹

### æ–¹å¼ 1ï¼šä¸€é”®éƒ¨ç½²ï¼ˆæ¨èï¼‰â­

ä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬å¿«é€Ÿéƒ¨ç½²ï¼š

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/tsaol/qwen3vl-on-aws.git
cd qwen3vl-on-aws

# 2. è¿è¡Œéƒ¨ç½²è„šæœ¬ï¼ˆå®‰è£…ä¾èµ–å’Œç¯å¢ƒï¼‰
bash deploy.sh

# 3. å¯åŠ¨æœåŠ¡
bash start_server.sh
```

**å°±è¿™ä¹ˆç®€å•ï¼** ğŸ‰ æœåŠ¡å°†åœ¨ `http://localhost:8000` å¯åŠ¨ã€‚

---

### æ–¹å¼ 2ï¼šæ‰‹åŠ¨å®‰è£…ï¼ˆé«˜çº§ç”¨æˆ·ï¼‰

å¦‚æœä½ æƒ³äº†è§£è¯¦ç»†æ­¥éª¤æˆ–è‡ªå®šä¹‰é…ç½®ï¼š

#### 1. å®‰è£…ä¾èµ–

ä½¿ç”¨ `uv` å·¥å…·ï¼ˆè¶…å¿«çš„ Python åŒ…ç®¡ç†å™¨ï¼‰ï¼š

```bash
# å®‰è£… uv (å¦‚æœè¿˜æ²¡å®‰è£…)
curl -LsSf https://astral.sh/uv/install.sh | sh

# æ›´æ–° PATHï¼ˆè®© shell èƒ½æ‰¾åˆ° uvï¼‰
source $HOME/.local/bin/env

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ (Python 3.10+)
uv venv --python 3.10 --seed

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate

# å®‰è£… vLLMï¼ˆè‡ªåŠ¨æ£€æµ‹ GPU åç«¯ï¼‰
uv pip install vllm --torch-backend=auto
```

#### 2. å¯åŠ¨ vLLM æœåŠ¡

```bash
# åŸºç¡€å¯åŠ¨å‘½ä»¤
vllm serve Qwen/Qwen3-VL-8B-Instruct \
  --port 8000 \
  --max-model-len 1024 \
  --gpu-memory-utilization 0.95
```

**å‚æ•°è¯´æ˜ï¼š**
- `--port 8000` - API æœåŠ¡ç«¯å£
- `--max-model-len 1024` - æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆè¾“å…¥+è¾“å‡º token æ€»æ•°ï¼‰
- `--gpu-memory-utilization 0.95` - ä½¿ç”¨ 95% çš„ GPU æ˜¾å­˜

---

### æµ‹è¯• API

æœåŠ¡å¯åŠ¨åï¼Œå¯ä»¥é€šè¿‡ OpenAI å…¼å®¹çš„ API è°ƒç”¨ï¼š

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen3-VL-8B-Instruct",
    "messages": [
      {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
    ]
  }'
```

## å®¢æˆ·ç«¯è°ƒç”¨ç¤ºä¾‹

æˆ‘ä»¬æä¾›äº†å¤šç§ç¼–ç¨‹è¯­è¨€çš„å®¢æˆ·ç«¯ç¤ºä¾‹ï¼Œè¯¦è§ [client_examples.md](client_examples.md)

### å¿«é€Ÿå¼€å§‹ - Python å®¢æˆ·ç«¯

```python
# ä½¿ç”¨æä¾›çš„ç¤ºä¾‹è„šæœ¬
python3 examples/python_client.py

# æˆ–è€…ä½¿ç”¨ OpenAI SDK
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"
)

response = client.chat.completions.create(
    model="Qwen/Qwen3-VL-8B-Instruct",
    messages=[{"role": "user", "content": "ä½ å¥½"}]
)

print(response.choices[0].message.content)
```

### æ”¯æŒçš„å®¢æˆ·ç«¯

- âœ… **cURL** - å‘½ä»¤è¡Œæµ‹è¯•
- âœ… **Python** (requests / OpenAI SDK) - [æŸ¥çœ‹ç¤ºä¾‹](examples/)
- âœ… **JavaScript/Node.js** (fetch / OpenAI SDK)
- âœ… **Go** - HTTP å®¢æˆ·ç«¯
- âœ… ä»»ä½•æ”¯æŒ OpenAI API çš„å®¢æˆ·ç«¯åº“

## âš ï¸ é«˜å¯ç”¨éƒ¨ç½²

**é‡è¦æç¤º**ï¼šå½“å‰çš„åŸºç¡€éƒ¨ç½²**ä¸æ˜¯é«˜å¯ç”¨**ï¼Œå­˜åœ¨ä»¥ä¸‹é£é™©ï¼š
- âŒ å•ç‚¹æ•…éšœ - è¿›ç¨‹å´©æºƒå¯¼è‡´æœåŠ¡ä¸­æ–­
- âŒ æ— è‡ªåŠ¨é‡å¯ - éœ€è¦äººå·¥å¹²é¢„
- âŒ SSH æ–­å¼€é£é™© - å¯èƒ½å¯¼è‡´è¿›ç¨‹ç»ˆæ­¢

### æ¨èï¼šå®‰è£… systemd æœåŠ¡

å®ç°è‡ªåŠ¨é‡å¯ã€å¼€æœºè‡ªå¯ã€æ•…éšœæ¢å¤ï¼š

```bash
# å®‰è£…é«˜å¯ç”¨æœåŠ¡
sudo bash install_service.sh

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
sudo systemctl status qwen3vl

# æŸ¥çœ‹å®æ—¶æ—¥å¿—
sudo journalctl -u qwen3vl -f
```

**å®Œæ•´é«˜å¯ç”¨éƒ¨ç½²æ–¹æ¡ˆ**è¯·å‚è€ƒï¼š[HIGH_AVAILABILITY.md](HIGH_AVAILABILITY.md)

## åŠ è½½ç§æœ‰æ¨¡å‹

### æ–¹æ³• 1ï¼šä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆæ¨èï¼‰

å¦‚æœæ¨¡å‹å·²ä¸‹è½½åˆ°æœ¬åœ°ï¼š

```bash
# ç›´æ¥æŒ‡å®šæœ¬åœ°è·¯å¾„
MODEL=/path/to/your/private/model bash start_server.sh
```

æˆ–ä¿®æ”¹ start_server.sh ä¸­çš„ MODEL å˜é‡ï¼š
```bash
MODEL="/data/models/my-private-qwen3vl"
```

### æ–¹æ³• 2ï¼šä½¿ç”¨ HuggingFace Tokenï¼ˆç§æœ‰ä»“åº“ï¼‰

å¦‚æœæ¨¡å‹åœ¨ HuggingFace ç§æœ‰ä»“åº“ï¼š

```bash
# è®¾ç½® HuggingFace Token
export HF_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxx"

# å¯åŠ¨æœåŠ¡
MODEL=your-org/private-model bash start_server.sh
```

æˆ–ä½¿ç”¨ huggingface-cli ä¸€æ¬¡æ€§ç™»å½•ï¼š
```bash
pip install huggingface-hub
huggingface-cli login
```

### æ–¹æ³• 3ï¼šä» S3/äº‘å­˜å‚¨åŠ è½½

```bash
# 1. ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
aws s3 sync s3://your-bucket/models/qwen3vl /data/models/qwen3vl

# 2. ä½¿ç”¨æœ¬åœ°è·¯å¾„å¯åŠ¨
MODEL=/data/models/qwen3vl bash start_server.sh
```

### æ–¹æ³• 4ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶

åˆ›å»º `model_config.env` æ–‡ä»¶ï¼š

```bash
# model_config.env
MODEL_PATH="/data/models/my-private-model"
HF_TOKEN="hf_xxxxx"  # å¦‚æœéœ€è¦
PORT=8000
MAX_MODEL_LEN=1024
GPU_MEMORY_UTIL=0.95
```

ä¿®æ”¹ start_server.sh åŠ è½½é…ç½®æ–‡ä»¶ï¼š
```bash
# åœ¨è„šæœ¬å¼€å¤´æ·»åŠ 
if [ -f "model_config.env" ]; then
    source model_config.env
fi
```

**å®‰å…¨æé†’**ï¼š
- ä¸è¦æŠŠ Token æäº¤åˆ° Git ä»“åº“
- å°† `model_config.env` æ·»åŠ åˆ° `.gitignore`
- ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ AWS Secrets Manager

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### è°ƒæ•´æ˜¾å­˜ä½¿ç”¨
```bash
# å¦‚æœé‡åˆ° OOM (Out of Memory)ï¼Œé™ä½æ˜¾å­˜åˆ©ç”¨ç‡
--gpu-memory-utilization 0.8
```

### å¢åŠ æœ€å¤§åºåˆ—é•¿åº¦
```bash
# å¤„ç†æ›´é•¿çš„æ–‡æœ¬
--max-model-len 4096
```

### å¯ç”¨å¼ é‡å¹¶è¡Œï¼ˆå¤š GPUï¼‰
```bash
# åœ¨å¤š GPU å®ä¾‹ä¸Šåˆ†å¸ƒæ¨¡å‹
--tensor-parallel-size 2  # ä½¿ç”¨ 2 ä¸ª GPU
```

## æ•…éšœæ’æŸ¥

### 1. CUDA æ‰¾ä¸åˆ°
```bash
# æ£€æŸ¥ CUDA æ˜¯å¦å®‰è£…
nvidia-smi

# æ£€æŸ¥ PyTorch CUDA æ”¯æŒ
python -c "import torch; print(torch.cuda.is_available())"
```

### 2. æ˜¾å­˜ä¸è¶³
- å‡å°‘ `--gpu-memory-utilization`
- å‡å°‘ `--max-model-len`
- ä½¿ç”¨æ›´å¤§çš„ GPU å®ä¾‹

### 3. æ¨¡å‹ä¸‹è½½æ…¢
```bash
# è®¾ç½® HuggingFace é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
```

## ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

å¯¹äºç”Ÿäº§ç¯å¢ƒï¼Œå»ºè®®ï¼š
1. ä½¿ç”¨ Docker å®¹å™¨åŒ–
2. é…ç½®è´Ÿè½½å‡è¡¡å™¨ï¼ˆALB/NLBï¼‰
3. è®¾ç½® Auto Scaling
4. å¯ç”¨ CloudWatch ç›‘æ§
5. é…ç½®æ—¥å¿—æ”¶é›†

## ç›¸å…³èµ„æº

- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [Qwen3-VL æ¨¡å‹å¡](https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct)
- [AWS GPU å®ä¾‹å®šä»·](https://aws.amazon.com/ec2/instance-types/)

## è®¸å¯è¯

MIT License
