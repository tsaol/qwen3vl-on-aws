# Qwen3-VL åœ¨ AWS ä¸Šçš„éƒ¨ç½²æŒ‡å—

è¿™ä¸ªä»“åº“åŒ…å«åœ¨ AWS ä¸Šéƒ¨ç½² Qwen3-VL-8B-Instruct æ¨¡å‹çš„è„šæœ¬å’Œæ–‡æ¡£ã€‚

## ç¯å¢ƒè¦æ±‚

- AWS EC2 å®ä¾‹ï¼ˆæ¨è G6e ç³»åˆ— GPU å®ä¾‹ï¼‰
- Python 3.10+
- NVIDIA GPUï¼ˆæ”¯æŒ CUDAï¼‰
- é€‰æ‹©Deep Learning Base AMI with Single CUDA (Ubuntu) 

## æ¨èçš„ AWS å®ä¾‹ç±»å‹

| å®ä¾‹ç±»å‹ | GPU | æ˜¾å­˜ |
|---------|-----|------|
| g6e.xlarge | 1x L40S | 48GB | 
| g6e.2xlarge | 1x L40S | 48GB | 

## å¿«é€Ÿå¼€å§‹

### æ–¹å¼ 1ï¼šä¸€é”®éƒ¨ç½²ï¼ˆæ¨èï¼‰â­

ä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬å¿«é€Ÿéƒ¨ç½²ï¼š

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/tsaol/qwen3vl-on-aws.git
cd qwen3vl-on-aws

# 2. è¿è¡Œéƒ¨ç½²è„šæœ¬ï¼ˆå®‰è£…ä¾èµ–å’Œç¯å¢ƒï¼‰
bash deploy.sh

# 3. å¯åŠ¨æœåŠ¡
bash start_server.sh
```

**å°±è¿™ä¹ˆç®€å•ï¼** ğŸ‰ æœåŠ¡å°†åœ¨ `http://localhost:8000` å¯åŠ¨ã€‚

---

### æ–¹å¼ 2ï¼šæ‰‹åŠ¨å®‰è£…ï¼ˆé«˜çº§ç”¨æˆ·ï¼‰

å¦‚æœä½ æƒ³äº†è§£è¯¦ç»†æ­¥éª¤æˆ–è‡ªå®šä¹‰é…ç½®ï¼š

#### 1. å®‰è£…ä¾èµ–

ä½¿ç”¨ `uv` å·¥å…·ï¼ˆè¶…å¿«çš„ Python åŒ…ç®¡ç†å™¨ï¼‰ï¼š

```bash
# å®‰è£… uv (å¦‚æœè¿˜æ²¡å®‰è£…)
curl -LsSf https://astral.sh/uv/install.sh | sh

# æ›´æ–° PATHï¼ˆè®© shell èƒ½æ‰¾åˆ° uvï¼‰
source $HOME/.local/bin/env

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ (Python 3.10+)
uv venv --python 3.10 --seed

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate

# å®‰è£… vLLMï¼ˆè‡ªåŠ¨æ£€æµ‹ GPU åç«¯ï¼‰
uv pip install vllm --torch-backend=auto
```

#### 2. å¯åŠ¨ vLLM æœåŠ¡

```bash
# åŸºç¡€å¯åŠ¨å‘½ä»¤
vllm serve Qwen/Qwen3-VL-8B-Instruct \
  --port 8000 \
  --max-model-len 1024 \
  --gpu-memory-utilization 0.95
```

**å‚æ•°è¯´æ˜ï¼š**
- `--port 8000` - API æœåŠ¡ç«¯å£
- `--max-model-len 1024` - æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆè¾“å…¥+è¾“å‡º token æ€»æ•°ï¼‰
- `--gpu-memory-utilization 0.95` - ä½¿ç”¨ 95% çš„ GPU æ˜¾å­˜

---

### æµ‹è¯• API

æœåŠ¡å¯åŠ¨åï¼Œä½¿ç”¨ç»Ÿä¸€æµ‹è¯•è„šæœ¬ï¼š

```bash
# äº¤äº’å¼é€‰æ‹©æµ‹è¯•ç±»å‹
bash test.sh
```

æˆ–ç›´æ¥ç”¨ curl æµ‹è¯•ï¼š

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen3-VL-8B-Instruct",
    "messages": [
      {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
    ]
  }'
```

---

## ğŸ”’ API Key è®¤è¯é…ç½®

vLLM æ”¯æŒåŸç”Ÿ API Key è®¤è¯ï¼Œå¯ç”¨åæ‰€æœ‰ API è¯·æ±‚éƒ½éœ€è¦æä¾›æœ‰æ•ˆçš„ Bearer Tokenã€‚

### ä¸ºä»€ä¹ˆéœ€è¦ API Keyï¼Ÿ

- âœ… **è®¿é—®æ§åˆ¶** - é˜²æ­¢æœªæˆæƒè®¿é—®
- âœ… **æˆæœ¬ç®¡ç†** - è¿½è¸ªå’Œæ§åˆ¶ API ä½¿ç”¨é‡
- âœ… **å®‰å…¨åˆè§„** - æ»¡è¶³ç”Ÿäº§ç¯å¢ƒå®‰å…¨è¦æ±‚
- âœ… **å¤šç§Ÿæˆ·éš”ç¦»** - æ”¯æŒä¸åŒå®¢æˆ·ç«¯ä½¿ç”¨ä¸åŒå¯†é’¥

### å¯ç”¨æ­¥éª¤

#### 1. ç”Ÿæˆå®‰å…¨çš„ API Key

```bash
# ä½¿ç”¨ Python ç”Ÿæˆéšæœºå¯†é’¥
python3 -c "import secrets; print(f'sk-qwen-{secrets.token_urlsafe(32)}')"
```

è¾“å‡ºç¤ºä¾‹ï¼š`sk-qwen-abc123def456...`

#### 2. é…ç½® systemd æœåŠ¡

ä½¿ç”¨æä¾›çš„è‡ªåŠ¨åŒ–è„šæœ¬ï¼š

```bash
# ç¼–è¾‘è„šæœ¬ï¼Œæ›¿æ¢ API_KEY ä¸ºä½ ç”Ÿæˆçš„å¯†é’¥
nano update-vllm-apikey.sh

# åœ¨ä¸¤å°å®ä¾‹ä¸Šæ‰§è¡Œï¼ˆå¦‚æœä½¿ç”¨å¤šå®ä¾‹éƒ¨ç½²ï¼‰
bash update-vllm-apikey.sh
```

æˆ–æ‰‹åŠ¨ä¿®æ”¹ `/etc/systemd/system/qwen3vl.service`ï¼š

```ini
ExecStart=/path/to/.venv/bin/vllm serve Qwen/Qwen3-VL-8B-Instruct \
  --port 8000 \
  --max-model-len 1024 \
  --gpu-memory-utilization 0.95 \
  --api-key YOUR_API_KEY_HERE
```

ç„¶åé‡å¯æœåŠ¡ï¼š

```bash
sudo systemctl daemon-reload
sudo systemctl restart qwen3vl
```

#### 3. éªŒè¯è®¤è¯ç”Ÿæ•ˆ

```bash
# æµ‹è¯• 1: ä¸å¸¦ API Keyï¼ˆåº”è¯¥å¤±è´¥ï¼‰
curl -w "\nHTTP: %{http_code}\n" \
  https://your-domain.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model": "Qwen/Qwen3-VL-8B-Instruct", "messages": [{"role": "user", "content": "ä½ å¥½"}]}'

# é¢„æœŸè¾“å‡ºï¼š{"error":"Unauthorized"} HTTP: 401

# æµ‹è¯• 2: å¸¦æ­£ç¡®çš„ API Keyï¼ˆåº”è¯¥æˆåŠŸï¼‰
curl https://your-domain.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{"model": "Qwen/Qwen3-VL-8B-Instruct", "messages": [{"role": "user", "content": "ä½ å¥½"}]}'

# é¢„æœŸè¾“å‡ºï¼šæ­£å¸¸çš„ JSON å“åº”
```

### å®¢æˆ·ç«¯ä½¿ç”¨

#### Python (OpenAI SDK)

```python
from openai import OpenAI

client = OpenAI(
    base_url="https://your-domain.com/v1",
    api_key="YOUR_API_KEY"  # æ›¿æ¢ä¸ºä½ çš„ API Key
)

response = client.chat.completions.create(
    model="Qwen/Qwen3-VL-8B-Instruct",
    messages=[{"role": "user", "content": "ä½ å¥½"}]
)

print(response.choices[0].message.content)
```

#### Python (requests)

```python
import requests

headers = {
    "Content-Type": "application/json",
    "Authorization": "Bearer YOUR_API_KEY"
}

data = {
    "model": "Qwen/Qwen3-VL-8B-Instruct",
    "messages": [{"role": "user", "content": "ä½ å¥½"}]
}

response = requests.post(
    "https://your-domain.com/v1/chat/completions",
    headers=headers,
    json=data
)

print(response.json())
```

#### cURL

```bash
curl https://your-domain.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{"model": "Qwen/Qwen3-VL-8B-Instruct", "messages": [...]}'
```

#### JavaScript/Node.js

```javascript
const response = await fetch('https://your-domain.com/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer YOUR_API_KEY'
  },
  body: JSON.stringify({
    model: 'Qwen/Qwen3-VL-8B-Instruct',
    messages: [{role: 'user', content: 'ä½ å¥½'}]
  })
});

const data = await response.json();
console.log(data.choices[0].message.content);
```

### å®‰å…¨æœ€ä½³å®è·µ

1. **ä¸è¦ç¡¬ç¼–ç  API Key** - ä½¿ç”¨ç¯å¢ƒå˜é‡
   ```bash
   export QWEN_API_KEY="sk-qwen-xxx"
   ```

2. **ä¸è¦æäº¤åˆ° Git** - æ·»åŠ åˆ° `.gitignore`
   ```
   .env
   config.yaml
   *_config.env
   ```

3. **å®šæœŸè½®æ¢å¯†é’¥** - å»ºè®®æ¯ 90 å¤©æ›´æ–°ä¸€æ¬¡

4. **ä½¿ç”¨ä¸åŒå¯†é’¥** - å¼€å‘/æµ‹è¯•/ç”Ÿäº§ç¯å¢ƒåˆ†ç¦»

5. **å¯ç”¨è®¿é—®æ—¥å¿—** - ç›‘æ§å¼‚å¸¸è®¿é—®æ¨¡å¼
   ```bash
   sudo journalctl -u qwen3vl -f | grep "Unauthorized"
   ```

### å¸¸è§é—®é¢˜

**Q: å¦‚ä½•ç¦ç”¨ API Key è®¤è¯ï¼Ÿ**
```bash
# ç§»é™¤ --api-key å‚æ•°ï¼Œé‡å¯æœåŠ¡
sudo nano /etc/systemd/system/qwen3vl.service
sudo systemctl daemon-reload
sudo systemctl restart qwen3vl
```

**Q: æ”¯æŒå¤šä¸ª API Key å—ï¼Ÿ**
vLLM åŸç”Ÿä»…æ”¯æŒå•ä¸ª API Keyã€‚å¦‚éœ€å¤šå¯†é’¥ç®¡ç†ï¼Œå»ºè®®ä½¿ç”¨ [LiteLLM Proxy](https://docs.litellm.ai/) æˆ– Nginx åå‘ä»£ç†ã€‚

**Q: å¿˜è®° API Key æ€ä¹ˆåŠï¼Ÿ**
```bash
# æŸ¥çœ‹å½“å‰é…ç½®çš„ API Key
sudo grep "api-key" /etc/systemd/system/qwen3vl.service
```

---

## å®¢æˆ·ç«¯è°ƒç”¨ç¤ºä¾‹

æˆ‘ä»¬æä¾›äº†å¤šç§ç¼–ç¨‹è¯­è¨€çš„å®¢æˆ·ç«¯ç¤ºä¾‹ï¼Œè¯¦è§ [client_examples.md](client_examples.md)

### å¿«é€Ÿå¼€å§‹ - Python å®¢æˆ·ç«¯

```python
# ä½¿ç”¨æä¾›çš„ç¤ºä¾‹è„šæœ¬
python3 examples/python_client.py

# æˆ–è€…ä½¿ç”¨ OpenAI SDK
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"
)

response = client.chat.completions.create(
    model="Qwen/Qwen3-VL-8B-Instruct",
    messages=[{"role": "user", "content": "ä½ å¥½"}]
)

print(response.choices[0].message.content)
```

### æ”¯æŒçš„å®¢æˆ·ç«¯

- âœ… **cURL** - å‘½ä»¤è¡Œæµ‹è¯•
- âœ… **Python** (requests / OpenAI SDK) - [æŸ¥çœ‹ç¤ºä¾‹](examples/)
- âœ… **JavaScript/Node.js** (fetch / OpenAI SDK)
- âœ… **Go** - HTTP å®¢æˆ·ç«¯
- âœ… ä»»ä½•æ”¯æŒ OpenAI API çš„å®¢æˆ·ç«¯åº“

## âš ï¸ é«˜å¯ç”¨éƒ¨ç½²

**é‡è¦æç¤º**ï¼šå½“å‰çš„åŸºç¡€éƒ¨ç½²**ä¸æ˜¯é«˜å¯ç”¨**ï¼Œå­˜åœ¨ä»¥ä¸‹é£é™©ï¼š
- âŒ å•ç‚¹æ•…éšœ - è¿›ç¨‹å´©æºƒå¯¼è‡´æœåŠ¡ä¸­æ–­
- âŒ æ— è‡ªåŠ¨é‡å¯ - éœ€è¦äººå·¥å¹²é¢„
- âŒ SSH æ–­å¼€é£é™© - å¯èƒ½å¯¼è‡´è¿›ç¨‹ç»ˆæ­¢

### æ¨èï¼šå®‰è£… systemd æœåŠ¡

å®ç°è‡ªåŠ¨é‡å¯ã€å¼€æœºè‡ªå¯ã€æ•…éšœæ¢å¤ï¼š

```bash
# å®‰è£…é«˜å¯ç”¨æœåŠ¡
sudo bash install_service.sh

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
sudo systemctl status qwen3vl

# æŸ¥çœ‹å®æ—¶æ—¥å¿—
sudo journalctl -u qwen3vl -f
```

**å®Œæ•´é«˜å¯ç”¨éƒ¨ç½²æ–¹æ¡ˆ**è¯·å‚è€ƒï¼š[GPU_HIGH_AVAILABILITY.md](GPU_HIGH_AVAILABILITY.md)

## åŠ è½½ç§æœ‰æ¨¡å‹

### æ–¹æ³• 1ï¼šä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆæ¨èï¼‰

å¦‚æœæ¨¡å‹å·²ä¸‹è½½åˆ°æœ¬åœ°ï¼š

```bash
# ç›´æ¥æŒ‡å®šæœ¬åœ°è·¯å¾„
MODEL=/path/to/your/private/model bash start_server.sh
```

æˆ–ä¿®æ”¹ start_server.sh ä¸­çš„ MODEL å˜é‡ï¼š
```bash
MODEL="/data/models/my-private-qwen3vl"
```

### æ–¹æ³• 2ï¼šä½¿ç”¨ HuggingFace Tokenï¼ˆç§æœ‰ä»“åº“ï¼‰

å¦‚æœæ¨¡å‹åœ¨ HuggingFace ç§æœ‰ä»“åº“ï¼š

```bash
# è®¾ç½® HuggingFace Token
export HF_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxx"

# å¯åŠ¨æœåŠ¡
MODEL=your-org/private-model bash start_server.sh
```

æˆ–ä½¿ç”¨ huggingface-cli ä¸€æ¬¡æ€§ç™»å½•ï¼š
```bash
pip install huggingface-hub
huggingface-cli login
```

### æ–¹æ³• 3ï¼šä» S3/äº‘å­˜å‚¨åŠ è½½

```bash
# 1. ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°
aws s3 sync s3://your-bucket/models/qwen3vl /data/models/qwen3vl

# 2. ä½¿ç”¨æœ¬åœ°è·¯å¾„å¯åŠ¨
MODEL=/data/models/qwen3vl bash start_server.sh
```

### æ–¹æ³• 4ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶

åˆ›å»º `model_config.env` æ–‡ä»¶ï¼š

```bash
# model_config.env
MODEL_PATH="/data/models/my-private-model"
HF_TOKEN="hf_xxxxx"  # å¦‚æœéœ€è¦
PORT=8000
MAX_MODEL_LEN=1024
GPU_MEMORY_UTIL=0.95
```

ä¿®æ”¹ start_server.sh åŠ è½½é…ç½®æ–‡ä»¶ï¼š
```bash
# åœ¨è„šæœ¬å¼€å¤´æ·»åŠ 
if [ -f "model_config.env" ]; then
    source model_config.env
fi
```

**å®‰å…¨æé†’**ï¼š
- ä¸è¦æŠŠ Token æäº¤åˆ° Git ä»“åº“
- å°† `model_config.env` æ·»åŠ åˆ° `.gitignore`
- ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ AWS Secrets Manager


## æ•…éšœæ’æŸ¥

### 1. CUDA æ‰¾ä¸åˆ°
```bash
# æ£€æŸ¥ CUDA æ˜¯å¦å®‰è£…
nvidia-smi

# æ£€æŸ¥ PyTorch CUDA æ”¯æŒ
python -c "import torch; print(torch.cuda.is_available())"
```

### 2. æ˜¾å­˜ä¸è¶³
- å‡å°‘ `--gpu-memory-utilization`
- å‡å°‘ `--max-model-len`
- ä½¿ç”¨æ›´å¤§çš„ GPU å®ä¾‹

### 3. æ¨¡å‹ä¸‹è½½æ…¢
```bash
# è®¾ç½® HuggingFace é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
```

## ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

å¯¹äºç”Ÿäº§ç¯å¢ƒï¼Œå»ºè®®ï¼š
1. ä½¿ç”¨ Docker å®¹å™¨åŒ–
2. é…ç½®è´Ÿè½½å‡è¡¡å™¨ï¼ˆALB/NLBï¼‰
3. è®¾ç½® Auto Scaling
4. å¯ç”¨ CloudWatch ç›‘æ§
5. é…ç½®æ—¥å¿—æ”¶é›†

## ç›¸å…³èµ„æº

- [vLLM å®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/)
- [Qwen3-VL æ¨¡å‹å¡](https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct)
- [AWS GPU å®ä¾‹å®šä»·](https://aws.amazon.com/ec2/instance-types/)

## è®¸å¯è¯

MIT License
