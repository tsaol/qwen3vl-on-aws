# vLLM éƒ¨ç½²æ–¹å¼

åŸºäº vLLM æ¨ç†æ¡†æ¶éƒ¨ç½² Qwen3-VL-8B-Instruct è§†è§‰è¯­è¨€æ¨¡å‹ã€‚

## ğŸ¯ ç‰¹ç‚¹

- **PagedAttention**: ç»†ç²’åº¦å†…å­˜ç®¡ç†ï¼Œé«˜æ•ˆçš„ KV Cache
- **é«˜ååé‡**: é€‚åˆæ‰¹é‡æ¨ç†å’Œé«˜å¹¶å‘åœºæ™¯
- **æˆç†Ÿç¨³å®š**: ç”Ÿäº§ç¯å¢ƒå¹¿æ³›ä½¿ç”¨
- **OpenAI å…¼å®¹**: å®Œå…¨å…¼å®¹ OpenAI API æ ¼å¼

## ğŸ“‹ é…ç½®å‚æ•°

- **ç«¯å£**: 8000
- **æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦**: 1,024 tokensï¼ˆé»˜è®¤ï¼‰
- **GPU æ˜¾å­˜åˆ©ç”¨ç‡**: 0.95
- **API Key è®¤è¯**: æ”¯æŒ

## ğŸš€ å¿«é€Ÿéƒ¨ç½²

### 1. éƒ¨ç½² vLLM

```bash
cd vllm
bash deploy.sh
```

è„šæœ¬ä¼šè‡ªåŠ¨ï¼š
- âœ… æ£€æŸ¥ GPU ç¯å¢ƒ
- âœ… å®‰è£… Python 3.10
- âœ… åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ `.venv`
- âœ… å®‰è£… vLLM å’Œä¾èµ–
- âœ… ä¸‹è½½æ¨¡å‹åˆ°ç¼“å­˜

### 2. å¯åŠ¨æœåŠ¡

```bash
bash start_server.sh
```

### 3. å®‰è£… systemd æœåŠ¡ï¼ˆæ¨èï¼‰

```bash
bash install_service.sh
```

æœåŠ¡ç®¡ç†å‘½ä»¤ï¼š
```bash
sudo systemctl start qwen3vl      # å¯åŠ¨
sudo systemctl stop qwen3vl       # åœæ­¢
sudo systemctl restart qwen3vl    # é‡å¯
sudo systemctl status qwen3vl     # æŸ¥çœ‹çŠ¶æ€
sudo journalctl -u qwen3vl -f     # æŸ¥çœ‹æ—¥å¿—
```

## ğŸ”’ API Key é…ç½®

ç”Ÿæˆå¹¶é…ç½® API Keyï¼š

```bash
# ç”Ÿæˆå¯†é’¥
python3 -c "import secrets; print(f'sk-qwen-{secrets.token_urlsafe(32)}')"

# æ›´æ–°é…ç½®
bash update_apikey.sh <your-api-key>
```

## ğŸ”§ è‡ªå®šä¹‰é…ç½®

ç¼–è¾‘ `start_server.sh` ä¿®æ”¹å¯åŠ¨å‚æ•°ï¼š

```bash
# æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆé»˜è®¤ 1024ï¼‰
MAX_MODEL_LEN=${MAX_MODEL_LEN:-1024}

# ä¿®æ”¹ä¸ºæ›´å¤§çš„å€¼ä»¥æ”¯æŒé•¿æ–‡æœ¬
MAX_MODEL_LEN=${MAX_MODEL_LEN:-32768}   # 32K
MAX_MODEL_LEN=${MAX_MODEL_LEN:-262144}  # 256K (æ¨¡å‹æœ€å¤§èƒ½åŠ›)

# GPU æ˜¾å­˜åˆ©ç”¨ç‡ï¼ˆé»˜è®¤ 0.95ï¼‰
GPU_MEMORY_UTIL=${GPU_MEMORY_UTIL:-0.95}

# ç«¯å£ï¼ˆé»˜è®¤ 8000ï¼‰
PORT=${PORT:-8000}
```

## ğŸ“Š æ€§èƒ½ç‰¹ç‚¹

| åœºæ™¯ | è¡¨ç° |
|------|------|
| çŸ­å¯¹è¯ | â­â­â­â­â­ ä¼˜ç§€ |
| æ‰¹é‡æ¨ç† | â­â­â­â­â­ ä¼˜ç§€ |
| é«˜å¹¶å‘ | â­â­â­â­â­ ä¼˜ç§€ |
| é•¿æ–‡æ¡£ | â­â­â­ éœ€é…ç½® max-model-len |
| è§†é¢‘ç†è§£ | â­â­â­ éœ€é…ç½® max-model-len |

## ğŸ§ª æµ‹è¯• API

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# æŸ¥çœ‹æ¨¡å‹
curl http://localhost:8000/v1/models

# èŠå¤©å®Œæˆï¼ˆéœ€è¦ API Keyï¼‰
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "model": "Qwen/Qwen3-VL-8B-Instruct",
    "messages": [{"role": "user", "content": "ä½ å¥½"}]
  }'
```

## ğŸ“ æ–‡ä»¶è¯´æ˜

- `deploy.sh` - éƒ¨ç½²è„šæœ¬ï¼Œå®‰è£… vLLM ç¯å¢ƒ
- `start_server.sh` - å¯åŠ¨ vLLM æœåŠ¡å™¨
- `qwen3vl.service` - systemd æœåŠ¡é…ç½®æ–‡ä»¶
- `install_service.sh` - å®‰è£… systemd æœåŠ¡
- `update_apikey.sh` - æ›´æ–° API Key é…ç½®

## ğŸ†š ä¸ SGLang å¯¹æ¯”

é€‰æ‹© vLLM å¦‚æœä½ éœ€è¦ï¼š
- âœ… æ›´é«˜çš„æ‰¹é‡æ¨ç†ååé‡
- âœ… æ›´æˆç†Ÿç¨³å®šçš„ç”Ÿäº§ç¯å¢ƒ
- âœ… æ›´å¥½çš„é«˜å¹¶å‘æ”¯æŒ
- âœ… æ›´å°‘çš„æ˜¾å­˜å ç”¨ï¼ˆç›¸åŒé…ç½®ä¸‹ï¼‰

é€‰æ‹© SGLang å¦‚æœä½ éœ€è¦ï¼š
- âœ… æ›´å¿«çš„é¦– token å»¶è¿Ÿ
- âœ… æ›´å¥½çš„å¤šè½®å¯¹è¯æ€§èƒ½
- âœ… æ›´é«˜æ•ˆçš„ KV Cache å¤ç”¨
- âœ… åŸç”Ÿæ”¯æŒæ¨¡å‹æœ€å¤§ä¸Šä¸‹æ–‡ï¼ˆ256Kï¼‰

## ğŸ“š æ›´å¤šæ–‡æ¡£

- [å®¢æˆ·ç«¯ä½¿ç”¨ç¤ºä¾‹](../docs/client_examples.md)
- [é«˜å¯ç”¨éƒ¨ç½²æ–¹æ¡ˆ](../docs/GPU_HIGH_AVAILABILITY.md)
- [LiteLLM ç½‘å…³æ–¹æ¡ˆ](../docs/LITELLM_HA_SOLUTIONS.md)
- [Python ç¤ºä¾‹ä»£ç ](../examples/)

## âš ï¸ å¸¸è§é—®é¢˜

### Q: ä¸ºä»€ä¹ˆé»˜è®¤ max-model-len åªæœ‰ 1024ï¼Ÿ

A: è¿™æ˜¯ä¸€ä¸ªä¿å®ˆçš„é»˜è®¤å€¼ï¼Œç¡®ä¿åœ¨å„ç§ GPU ä¸Šéƒ½èƒ½è¿è¡Œã€‚Qwen3-VL å®é™…æ”¯æŒ 256K tokensï¼Œä½ å¯ä»¥æ ¹æ® GPU æ˜¾å­˜è°ƒæ•´è¿™ä¸ªå€¼ã€‚

### Q: å¦‚ä½•æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Ÿ

A: ä¿®æ”¹ `start_server.sh` ä¸­çš„ `MAX_MODEL_LEN` å‚æ•°ï¼Œç„¶åé‡å¯æœåŠ¡ï¼š
```bash
MAX_MODEL_LEN=262144  # 256K
```

### Q: API Key å­˜å‚¨åœ¨å“ªé‡Œï¼Ÿ

A: API Key é…ç½®åœ¨ systemd æœåŠ¡æ–‡ä»¶ `qwen3vl.service` çš„ `ExecStart` å‚æ•°ä¸­ã€‚

### Q: å¦‚ä½•åˆ‡æ¢åˆ° SGLangï¼Ÿ

A: ä½¿ç”¨è¿ç§»å·¥å…·ï¼š
```bash
cd ..
bash tools/migrate_to_sglang.sh
```

## ğŸ“ æŠ€æœ¯æ”¯æŒ

- GitHub Issues: https://github.com/tsaol/qwen3vl-on-aws/issues
- vLLM æ–‡æ¡£: https://docs.vllm.ai/
